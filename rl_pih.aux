\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{peg_personal_icra_2010}
\citation{peg_personal_icra_2010}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013}
\citation{sol_pdg_pbd_2014}
\citation{Kronander2015}
\citation{hybrid_1992}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\citation{search_strategies_icra_2001}
\citation{peg_imcssd_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Peg-in-hole}{2}{subsection.1.1}}
\citation{intuitive_peg_isr_2013}
\citation{compliant_manip_icra_2008}
\citation{online_gpr_icra_2014}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\citation{peg_personal_icra_2010}
\citation{hybrid_1992}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013}
\citation{sol_pdg_pbd_2014}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Actor-Critic \& Fitted Reinforcement Learning}{3}{subsection.1.2}}
\citation{search_strategies_icra_2001}
\citation{peg_imcssd_2015}
\citation{intuitive_peg_isr_2013}
\citation{compliant_manip_icra_2008}
\citation{online_gpr_icra_2014}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiment methods}{4}{section.2}}
\newlabel{ch4:experiment}{{2}{4}{Experiment methods}{section.2}{}}
\citation{Bergman99recursivebayesian}
\citation{NIPS2002_2319}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The experimental setup. \textit  {Top-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit  {Top-right:} Dimensions of the the wall and socket. \textit  {Bottom:} Three different power sockets, only socket A and B are used for data collection, socket C is purely used for evaluating the generalisation of the learned policy.}}{5}{figure.1}}
\newlabel{fig:search_task_setup}{{1}{5}{The experimental setup. \textit {Top-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit {Top-right:} Dimensions of the the wall and socket. \textit {Bottom:} Three different power sockets, only socket A and B are used for data collection, socket C is purely used for evaluating the generalisation of the learned policy}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Human holding the cylinder plug holder, which is equipped with OptiTrack markers.}}{5}{figure.2}}
\newlabel{fig:plug_cylinder}{{2}{5}{Human holding the cylinder plug holder, which is equipped with OptiTrack markers}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.1}Belief state}{5}{subsubsection.2.0.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textit  {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit  {Right}: \textbf  {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf  {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. }}{6}{figure.3}}
\newlabel{fig:PMF}{{3}{6}{\textit {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit {Right}: \textbf {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one}{figure.3}{}}
\citation{Kronander2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Participants and experiment protocol}{7}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded.}}{7}{figure.4}}
\newlabel{fig:experiment_design}{{4}{7}{Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded}{figure.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Preliminary results}{7}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning Actor and Critic}{7}{section.3}}
\newlabel{ch4:learning-value-actor}{{3}{7}{Learning Actor and Critic}{section.3}{}}
\citation{Atkeson97locallyweighted}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textit  {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated. \textit  {Bottom:} Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red and blue. The second later indicates which socket is used, see Figure \ref  {fig:experiment_design}.}}{8}{figure.5}}
\newlabel{fig:experiment_setup_data}{{5}{8}{\textit {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated. \textit {Bottom:} Time taken for the teachers to accomplish the PiH once the socket is localised. Group A and B are depicted in red and blue. The second later indicates which socket is used, see Figure \ref {fig:experiment_design}}{figure.5}{}}
\newlabel{eq:value_function}{{1}{8}{Learning Actor and Critic}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Actor \& Critic}{8}{subsection.3.1}}
\newlabel{eq:GMM}{{2}{8}{Actor \& Critic}{equation.3.2}{}}
\citation{EGW05}
\citation{NIPS2008_3501,EGW05,Riedmiller2005}
\citation{sutton1998reinforcement}
\citation{p_search_surv_2011}
\citation{peter_nac_2008}
\newlabel{eq:W}{{3}{9}{Actor \& Critic}{equation.3.3}{}}
\newlabel{eq:lwr_predict}{{4}{9}{Actor \& Critic}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fitted policy evaluation and improvement}{9}{subsection.3.2}}
\newlabel{sec:fpe}{{3.2}{9}{Fitted policy evaluation and improvement}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Policy evaluation}{9}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Policy improvement}{9}{section*.3}}
\newlabel{eq:disc_return}{{5}{9}{Policy improvement}{equation.3.5}{}}
\newlabel{eq:expected_reward}{{6}{9}{Policy improvement}{equation.3.6}{}}
\newlabel{eq:grad_log_cost}{{7}{9}{Policy improvement}{equation.3.7}{}}
\newlabel{eq:advantage_f}{{8}{9}{Policy improvement}{equation.3.8}{}}
\@writefile{toc}{\contentsline {paragraph}{2D example fitted policy evaluation and improvement}{10}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Belief state fitted policy evaluation}{10}{section*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Fitted policy evaluation \& improvement example. \textit  {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit  {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\boldsymbol  {\theta }}(x)$ is plotted in blue and trajectories generated by the policy $\mathbb  {E}\{\pi _{\boldsymbol  {\theta }}(\mathaccentV {dot}05F{x}|x)\}$ in orange. \textit  {Top-right} \textit  {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit  {Bottom-right} \textit  {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function. }}{10}{figure.6}}
\newlabel{fig:fpe_example}{{6}{10}{Fitted policy evaluation \& improvement example. \textit {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\Param }(\X )$ is plotted in blue and trajectories generated by the policy $\mathbb {E}\{\pi _{\Param }(\U |\X )\}$ in orange. \textit {Top-right} \textit {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit {Bottom-right} \textit {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces LWR value function approximate $\mathaccentV {hat}05E{V}^{\pi }(\mathaccentV {hat}05E{x})$ for the most likely state $\mathaccentV {hat}05E{x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. }}{11}{figure.7}}
\newlabel{fig:ch4:Figure1}{{7}{11}{LWR value function approximate $\hat {V}^{\pi }(\hat {x})$ for the most likely state $\hat {x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$.}}{11}{figure.8}}
\newlabel{fig:best_worst_traj}{{8}{11}{Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$}{figure.8}{}}
\citation{gesture_calinon_2010}
\citation{gmr_2004}
\@writefile{toc}{\contentsline {section}{\numberline {4}Control architecture}{12}{section.4}}
\newlabel{ch4:control_architecture}{{4}{12}{Control architecture}{section.4}{}}
\newlabel{eq:gmm_conditional}{{9}{12}{Control architecture}{equation.4.9}{}}
\newlabel{eq:alpha_eq}{{10}{12}{Control architecture}{equation.4.10}{}}
\newlabel{eq:alpha_expectation}{{11}{12}{Control architecture}{equation.4.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Q-EM and GMM policy vector fields. \textit  {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit  {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.}}{12}{figure.9}}
\newlabel{fig:policy_vf}{{9}{12}{Q-EM and GMM policy vector fields. \textit {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Robot Implementation}{13}{subsection.4.1}}
\newlabel{eq:modulation}{{12}{13}{Robot Implementation}{equation.4.12}{}}
\newlabel{eq:prop_speed}{{13}{13}{Robot Implementation}{equation.4.13}{}}
\newlabel{eq:torque_control}{{14}{13}{Robot Implementation}{equation.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The KUKA LWR is a 7 Degree Of Freedom (DoF) robot, we illustrate in red each joint, which is controlled at a rate of 1kHz via an ethernet cable. The KUKA API provides a command interface to the stiffness, damping, position and torque variables of each joint.}}{13}{figure.10}}
\newlabel{fig:kuka}{{10}{13}{The KUKA LWR is a 7 Degree Of Freedom (DoF) robot, we illustrate in red each joint, which is controlled at a rate of 1kHz via an ethernet cable. The KUKA API provides a command interface to the stiffness, damping, position and torque variables of each joint}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Control architecture. The PMF (belief) receives a measured velocity, $\mathaccentV {dot}05F{\mathaccentV {tilde}07E{x}}$, and a sensor measurement $\mathaccentV {tilde}07E{y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref  {eq:prop_speed}.}}{14}{figure.11}}
\newlabel{fig:control_flow}{{11}{14}{Control architecture. The PMF (belief) receives a measured velocity, $\dot {\tilde {x}}$, and a sensor measurement $\tilde {y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller, Equation \ref {eq:prop_speed}}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{14}{section.5}}
\newlabel{ch4:results}{{5}{14}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Distance taken to reach the socket's edge (Qualitative)}{14}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Three simulated search experiments. \textbf  {Experiment 1:} Three start positions are considered: \textit  {Left}, \textit  {Center} and \textit  {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf  {Experiment 2:} Two cases are considered: \textit  {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit  {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit  {Case 1}. \textbf  {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search.}}{15}{figure.12}}
\newlabel{fig:box_exp_sim}{{12}{15}{Three simulated search experiments. \textbf {Experiment 1:} Three start positions are considered: \textit {Left}, \textit {Center} and \textit {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. In the second row of Experiment 1, we illustrate the trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textbf {Experiment 2:} Two cases are considered: \textit {Case 1} blue, the initial belief state (circle) is fixed facing the left edge of the wall and the true location (diamond) is facing the socket. \textit {Case 2} pink, the initial belief state (circle) is fixed to the right facing the edge of the wall and the true location is the left edge of the wall. In the second row, the trajectories are plotted for \textit {Case 1}. \textbf {Experiment 3:} A 150 start locations are deterministically generated from a grid in the start area. In the second row, we plot the distribution of the areas visited by the true position during the search}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function.}}{16}{figure.13}}
\newlabel{fig:first_contact}{{13}{16}{First contact with the wall, during experiment 1. (a) Contact distribution for initial condition ``Center'' . (b) Contact distribution for initial condition was ``Right''. The ellipses correspond to two standard deviations of a fitted Gaussian function}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Distance taken to reach the socket's edge (Quantitative)}{16}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Distance travelled until the socket's edge is reached. a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref  {fig:box_exp_sim}, \textit  {top left}. The Q-EM method is always better than the other methods, in terms of distance. b) Results of the two initial conditions depicted in Figure \ref  {fig:box_exp_sim}, \textit  {top middle}, both the true position and most likely state are fixed. The Q-EM method always improves on the GMM. }}{17}{figure.14}}
\newlabel{fig:three_searches}{{14}{17}{Distance travelled until the socket's edge is reached. a) Three groups correspond to the initial conditions: Center, Left and Right depicted in Figure \ref {fig:box_exp_sim}, \textit {top left}. The Q-EM method is always better than the other methods, in terms of distance. b) Results of the two initial conditions depicted in Figure \ref {fig:box_exp_sim}, \textit {top middle}, both the true position and most likely state are fixed. The Q-EM method always improves on the GMM}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Distance travelled until the socket's edge is reached. Results corresponding to Experiment 3, Figure \ref  {fig:box_exp_sim}, \textit  {top right}. Again the Q-EM method is better, but at a less significant level.}}{17}{figure.15}}
\newlabel{fig:three_searches_exp3}{{15}{17}{Distance travelled until the socket's edge is reached. Results corresponding to Experiment 3, Figure \ref {fig:box_exp_sim}, \textit {top right}. Again the Q-EM method is better, but at a less significant level}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Demonstrations of teacher \# 5. The teacher demonstrates a preference}}{17}{figure.16}}
\newlabel{fig:subj_5_traj}{{16}{17}{Demonstrations of teacher \# 5. The teacher demonstrates a preference}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Importance of data}{17}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted.}}{18}{figure.17}}
\newlabel{fig:value_function_subj_5}{{17}{18}{Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The illustrated transparency of the Gaussian functions is proportional to their weight. \textit  {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function being higher in the top right corner region, see Figure \ref  {fig:value_function_subj_5}. \textit  {Center column}: The original data of the teacher went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the wall. We can also see from the two plots of the Q-EM parameters that they then follow the paths encoded by the value function. \textit  {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.}}{18}{figure.18}}
\newlabel{fig:gmm_exp4}{{18}{18}{Marginalised Gaussian Mixture parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. The illustrated transparency of the Gaussian functions is proportional to their weight. \textit {Left column}: The Gaussian functions of the Q-EM have shifted from the left corner to the right. This is a result of the value function being higher in the top right corner region, see Figure \ref {fig:value_function_subj_5}. \textit {Center column}: The original data of the teacher went quite far back which results in a Gaussian function given a direction which moves away from the wall (green arrow), whilst in the case of the Q-EM parameters this effect is reduced and moved closer towards the wall. We can also see from the two plots of the Q-EM parameters that they then follow the paths encoded by the value function. \textit {Right column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Generalisation}{19}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.}}{19}{figure.19}}
\newlabel{fig:experiment4_stats}{{19}{19}{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy}{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit  {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. }}{20}{figure.20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{20}{figure.20}}
\newlabel{fig:experiment5_traj}{{20}{20}{Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref  {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. }}{20}{figure.21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{20}{figure.21}}
\newlabel{fig:experiment5_stats}{{21}{20}{Distance taken to reach the socket's edge. For the Fixed setup (see Figure \ref {fig:experiment5_traj}) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy}{figure.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Distance taken to connect the plug to the socket}{21}{subsection.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces 25 search trajectories for each of the three search policies for socket A. }}{21}{figure.22}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{21}{figure.22}}
\newlabel{fig:real_policy}{{22}{21}{25 search trajectories for each of the three search policies for socket A}{figure.22}{}}
\citation{Chambrier2014}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection.}}{22}{figure.23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{22}{figure.23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{22}{figure.23}}
\newlabel{fig:real_pictures}{{23}{22}{KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. (b) Same initial condition as in (a) but with socket C. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection}{figure.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion \& Conclusion}{22}{section.6}}
\newlabel{ch4:conclusion}{{6}{22}{Discussion \& Conclusion}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Distance taken to connect plug to socket once the socket is localised. (a) \textbf  {Socket A}. The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group BA first gave demonstrations on Socket B before giving demonstrations on Socket A. Group BA is better than Group AA at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (b) \textbf  {Socket B}. Both Groups AB and BB are similar in terms of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish the task. }}{23}{figure.24}}
\newlabel{fig:real_statistics}{{24}{23}{Distance taken to connect plug to socket once the socket is localised. (a) \textbf {Socket A}. The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group BA first gave demonstrations on Socket B before giving demonstrations on Socket A. Group BA is better than Group AA at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (b) \textbf {Socket B}. Both Groups AB and BB are similar in terms of the distance they took to insert the plug into the socket and as was the case for (a), the search policies travel less to accomplish the task}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket.}}{23}{figure.25}}
\newlabel{fig:real_statistics2}{{25}{23}{Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket}{figure.25}{}}
\bibstyle{elsarticle-harv}
\bibdata{bib/peg_hole.bib,bib/pomdp.bib,bib/RL.bib,ch3-citations.bib,bib/DT.bib,bib/MLMF.bib,bib/spatial_navigation.bib}
\bibcite{sol_pdg_pbd_2014}{{1}{2014}{{Abu-Dakka et~al.}}{{Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger, and Ude}}}
\bibcite{Atkeson97locallyweighted}{{2}{1997}{{Atkeson et~al.}}{{Atkeson, Moore, and Schaal}}}
\bibcite{peg_imcssd_2015}{{3}{2015}{{Bdiwi et~al.}}{{Bdiwi, Winkler, Jokesch, and Suchy}}}
\bibcite{Bergman99recursivebayesian}{{4}{1999}{{Bergman and Bergman}}{{}}}
\bibcite{gesture_calinon_2010}{{5}{2010}{{Calinon et~al.}}{{Calinon, D'halluin, Sauser, Caldwell, and Billard}}}
\bibcite{Chambrier2014}{{6}{2014}{{Chambrier and Billard}}{{}}}
\bibcite{online_gpr_icra_2014}{{7}{2014}{{Cheng and Chen}}{{}}}
\bibcite{search_strategies_icra_2001}{{8}{2001}{{Chhatpar and Branicky}}{{}}}
\bibcite{p_search_surv_2011}{{9}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{EGW05}{{10}{2005}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{hybrid_1992}{{11}{1992}{{Fisher and Mujtaba}}{{}}}
\bibcite{learn_admittance_icra_1994}{{12}{1994}{{Gullapalli et~al.}}{{Gullapalli, Barto, and Grupen}}}
\bibcite{learn_force_c_icirs_2011}{{13}{2011}{{Kalakrishnan et~al.}}{{Kalakrishnan, Righetti, Pastor, and Schaal}}}
\bibcite{compliant_manip_icra_2008}{{14}{2008}{{kook Yun}}{{}}}
\bibcite{peg_personal_icra_2010}{{15}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{trans_workpiece_icra_2013}{{16}{2013}{{Nemec et~al.}}{{Nemec, Abu-Dakka, Ridge, Ude, Jorgensen, Savarimuthu, Jouffroy, Petersen, and Kr\"uger}}}
\bibcite{NIPS2008_3501}{{17}{2009}{{Neumann and Peters}}{{}}}
\bibcite{intuitive_peg_isr_2013}{{18}{2013}{{Park et~al.}}{{Park, Bae, Park, Baeg, and Park}}}
\bibcite{peter_nac_2008}{{19}{2008}{{Peters and Schaal}}{{}}}
\bibcite{Riedmiller2005}{{20}{2005}{{Riedmiller}}{{}}}
\bibcite{NIPS2002_2319}{{21}{2003}{{Roy and Gordon}}{{}}}
\bibcite{Schaal04learningmovement}{{22}{2004}{{Schaal et~al.}}{{Schaal, Peters, Nakanishi, and Ijspeert}}}
\bibcite{gmr_2004}{{23}{2004}{{Sung}}{{}}}
\bibcite{sutton1998reinforcement}{{24}{1998}{{Sutton and Barto}}{{}}}
\bibcite{fast_peg_pbd_icmc_2014}{{25}{2014}{{Yang et~al.}}{{Yang, Lin, Song, Nemec, Ude, Buch, Kr√ºger, and Savarimuthu}}}
