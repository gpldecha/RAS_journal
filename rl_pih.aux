\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\emailauthor{guillaume.dechambrier@epfl.ch}{Guillaume de Chambrier\corref {cor1}\fnref {fn1}}
\emailauthor{aude.billard@epfl.ch}{Aude Billard}
\citation{PBVI}
\citation{pomdp_peg_icra_2014}
\citation{toussain_2015}
\citation{Lauri2016}
\citation{Chambrier2014}
\Newlabel{rvt}{a}
\Newlabel{cor1}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\citation{peg_personal_icra_2010}
\citation{search_strategies_icra_2001}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013,sol_pdg_pbd_2014}
\citation{Kronander2015}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\citation{sutton1998reinforcement}
\citation{rl_ac_surv_2012}
\citation{stable_FA_gordon_1995}
\citation{kernel_rl_ormoneit_2002}
\citation{fvi_uav_2010}
\citation{EGW05}
\citation{fqi_nips_peter_2009}
\citation{Riedmiller2005}
\citation{NAC_2008}
\citation{rl_gmm_2010}
\citation{Lange_riedmiller_2010}
\citation{mnih-dqn-2015}
\citation{DRQ_AAAI_2015}
\citation{approx_rl_overview_2011}
\citation{RL_state_art_2012}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}}
\newlabel{sec:related_work}{{2}{2}{Related work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Peg-in-hole}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Actor-Critic \& Fitted Reinforcement Learning}{2}{subsection.2.2}}
\citation{Bergman99recursivebayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Peg-in-Hole search task setup}. \textit  {Top-left}: Three different sockets are used, socket A will be only used to gather training data whilst socket B and C will be used evaluation purposes: \textit  {Bottom-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit  {Top-right:} Dimensions of the the wall and socket, and the orange area illustrates the possible starting locations. \textit  {Bottom-right:} The KUKA LWR robot is equipped with a peg holder mounted with an ATI force torque sensor, it is reproducing a search and connection policy learned from the human demonstrations.}}{3}{figure.1}}
\newlabel{fig:search_task_setup}{{1}{3}{\textbf {Peg-in-Hole search task setup}. \textit {Top-left}: Three different sockets are used, socket A will be only used to gather training data whilst socket B and C will be used evaluation purposes: \textit {Bottom-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit {Top-right:} Dimensions of the the wall and socket, and the orange area illustrates the possible starting locations. \textit {Bottom-right:} The KUKA LWR robot is equipped with a peg holder mounted with an ATI force torque sensor, it is reproducing a search and connection policy learned from the human demonstrations}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiment methods}{3}{section.3}}
\newlabel{sec:experiment_methods}{{3}{3}{Experiment methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Belief state}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Participants and experiment protocol}{3}{subsection.3.2}}
\citation{Kronander2015}
\citation{Atkeson97locallyweighted}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textit  {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit  {Right}: \textbf  {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf  {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. }}{4}{figure.2}}
\newlabel{fig:PMF}{{2}{4}{\textit {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit {Right}: \textbf {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Learning Actor and Critic}{4}{section.4}}
\newlabel{sec:learning-value-actor}{{4}{4}{Learning Actor and Critic}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded.}}{4}{figure.3}}
\newlabel{fig:experiment_design}{{3}{4}{Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded}{figure.3}{}}
\newlabel{eq:value_function}{{1}{4}{Learning Actor and Critic}{equation.4.1}{}}
\citation{EGW05}
\citation{NIPS2008_3501,EGW05,Riedmiller2005}
\citation{p_search_surv_2011}
\citation{p_search_surv_2011}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated.}}{5}{figure.4}}
\newlabel{fig:experiment_setup_data}{{4}{5}{\textit {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Actor \& Critic}{5}{subsection.4.1}}
\newlabel{eq:GMM}{{2}{5}{Actor \& Critic}{equation.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fitted Policy Iteration}{5}{subsection.4.2}}
\newlabel{sec:FPI}{{4.2}{5}{Fitted Policy Iteration}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Policy evaluation}{5}{section*.1}}
\newlabel{alg:fpe}{{1}{5}{Policy evaluation}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Fitted Policy Evaluation}}{5}{algocf.1}}
\@writefile{toc}{\contentsline {paragraph}{Policy improvement}{5}{section*.2}}
\newlabel{eq:disc_return}{{3}{5}{Policy improvement}{equation.4.3}{}}
\newlabel{eq:expected_reward}{{4}{5}{Policy improvement}{equation.4.4}{}}
\newlabel{eq:grad_log_cost}{{5}{5}{Policy improvement}{equation.4.5}{}}
\citation{Bishop_2006}
\citation{Bishop_2006}
\citation{peter_nac_2008}
\newlabel{fig:Q-EM}{{2}{6}{Policy improvement}{equation.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Q-EM Maximisation of the GMM parameters. We used the same notation and derivation as in \cite  [Chap. 9.2.2]{Bishop_2006}, where $\gamma _k(\mathbf  {x}^{[j]})$ is he responsibility factor, denoting the probability that data point $\mathbf  {x}^{[j]} = [\mathaccentV {dot}05F{x}^{[j]},b^{[j]}]^{\mathrm  {T}}$ belongs to Gaussian function $k$.}}{6}{figure.5}}
\newlabel{eq:advantage_f}{{6}{6}{Policy improvement}{equation.4.6}{}}
\@writefile{toc}{\contentsline {paragraph}{2D example fitted policy iteration}{6}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Fitted policy evaluation \& improvement example. \textit  {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit  {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\boldsymbol  {\theta }}(x)$ is plotted in blue and trajectories generated by the policy $\mathbb  {E}\{\pi _{\boldsymbol  {\theta }}(\mathaccentV {dot}05F{x}|x)\}$ in orange. \textit  {Top-right} \textit  {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit  {Bottom-right} \textit  {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function. }}{6}{figure.6}}
\newlabel{fig:fpe_example}{{6}{6}{Fitted policy evaluation \& improvement example. \textit {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\Param }(\X )$ is plotted in blue and trajectories generated by the policy $\mathbb {E}\{\pi _{\Param }(\U |\X )\}$ in orange. \textit {Top-right} \textit {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit {Bottom-right} \textit {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function}{figure.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Belief state fitted policy evaluation}{6}{section*.4}}
\citation{gesture_calinon_2010}
\citation{gmr_2004}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textit  {Left}: LWR value function approximate $\mathaccentV {hat}05E{V}^{\pi }(\mathaccentV {hat}05E{x})$ for the most likely state $\mathaccentV {hat}05E{x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. \textit  {Middle-right}: Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$. }}{7}{figure.7}}
\newlabel{fig:ch4:Figure1}{{7}{7}{\textit {Left}: LWR value function approximate $\hat {V}^{\pi }(\hat {x})$ for the most likely state $\hat {x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. \textit {Middle-right}: Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Control architecture}{7}{section.5}}
\newlabel{sec:control_architecture}{{5}{7}{Control architecture}{section.5}{}}
\newlabel{eq:gmm_conditional}{{7}{7}{Control architecture}{equation.5.7}{}}
\newlabel{eq:alpha_eq}{{8}{7}{Control architecture}{equation.5.8}{}}
\newlabel{eq:alpha_expectation}{{9}{7}{Control architecture}{equation.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Robot Implementation}{7}{subsection.5.1}}
\newlabel{eq:modulation}{{10}{7}{Robot Implementation}{equation.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Q-EM and GMM policy vector fields. \textit  {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit  {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.}}{8}{figure.8}}
\newlabel{fig:policy_vf}{{8}{8}{Q-EM and GMM policy vector fields. \textit {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{8}{section.6}}
\newlabel{sec:results}{{6}{8}{Results}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Control architecture. The PMF (belief) receives a measured velocity, $\mathaccentV {dot}05F{\mathaccentV {tilde}07E{x}}$, and a sensor measurement $\mathaccentV {tilde}07E{y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller.}}{8}{figure.9}}
\newlabel{fig:control_flow}{{9}{8}{Control architecture. The PMF (belief) receives a measured velocity, $\dot {\tilde {x}}$, and a sensor measurement $\tilde {y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Distance taken to reach the socket's edge}{8}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Importance of data}{9}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Two simulated search experiments. \textbf  {Experiment 1:} \textit  {Top-left}: Three start positions are considered: \textit  {Left}, \textit  {Center} and \textit  {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. \textit  {Bottom-left}: Trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textit  {Bottom-right:} Distribution of first contact point giving the center initial starting condition. \textit  {Top-right}: Distribution of distance travelled until the socket's edge was localised. The Q-EM policy is always the best. \textbf  {Experiment 2}: \textit  {Left}: Distribution of the visited regions during the search for the socket's edge. The Q-EM policy's distribution is more centred along the axis $z=0$. \textit  {Right:} Time taken to find the socket, the search algorithms are better than the humans with the exception of group BA.}}{10}{figure.10}}
\newlabel{fig:experiment12}{{10}{10}{Two simulated search experiments. \textbf {Experiment 1:} \textit {Top-left}: Three start positions are considered: \textit {Left}, \textit {Center} and \textit {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. \textit {Bottom-left}: Trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textit {Bottom-right:} Distribution of first contact point giving the center initial starting condition. \textit {Top-right}: Distribution of distance travelled until the socket's edge was localised. The Q-EM policy is always the best. \textbf {Experiment 2}: \textit {Left}: Distribution of the visited regions during the search for the socket's edge. The Q-EM policy's distribution is more centred along the axis $z=0$. \textit {Right:} Time taken to find the socket, the search algorithms are better than the humans with the exception of group BA}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Experiment 3} \textit  {Top-left}: Demonstrations of teacher \# 5. \textit  {Bottom-left} Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted. \textit  {Middle-column}: Most likely state parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. \textit  {Right-column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.}}{11}{figure.11}}
\newlabel{fig:experiment3}{{11}{11}{\textbf {Experiment 3} \textit {Top-left}: Demonstrations of teacher \# 5. \textit {Bottom-left} Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted. \textit {Middle-column}: Most likely state parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. \textit {Right-column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.}}{12}{figure.12}}
\newlabel{fig:experiment3_stats}{{12}{12}{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Generalisation}{12}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Distance taken to connect the plug to the socket}{12}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Experiment 4} Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit  {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. }}{13}{figure.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{13}{figure.13}}
\newlabel{fig:experiment4}{{13}{13}{\textbf {Experiment 4} Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textit  {Left}: 25 search trajectories for each of the three search policies for socket A. \textit  {Right} KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. \textit  {Bottom}: Socket C, same initial condition. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection.}}{13}{figure.14}}
\newlabel{fig:real_pictures}{{14}{13}{\textit {Left}: 25 search trajectories for each of the three search policies for socket A. \textit {Right} KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. \textit {Bottom}: Socket C, same initial condition. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection}{figure.14}{}}
\citation{Chambrier2014}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion \& Conclusion}{14}{section.7}}
\newlabel{sec:conclusion}{{7}{14}{Discussion \& Conclusion}{section.7}{}}
\bibstyle{elsarticle-harv}
\bibdata{bib/ras_fpi.bib}
\bibcite{sol_pdg_pbd_2014}{{1}{2014}{{Abu-Dakka et~al.}}{{Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger, and Ude}}}
\bibcite{rl_gmm_2010}{{2}{2010}{{Agostini and Celaya}}{{}}}
\bibcite{Atkeson97locallyweighted}{{3}{1997}{{Atkeson et~al.}}{{Atkeson, Moore, and Schaal}}}
\bibcite{Bergman99recursivebayesian}{{4}{1999}{{Bergman and Bergman}}{{}}}
\bibcite{Bishop_2006}{{5}{2006}{{Bishop}}{{}}}
\bibcite{fvi_uav_2010}{{6}{2010}{{Bou-Ammar et~al.}}{{Bou-Ammar, Voos, and Ertel}}}
\bibcite{approx_rl_overview_2011}{{7}{2011}{{Busoniu et~al.}}{{Busoniu, Ernst, Schutter, and Babuska}}}
\bibcite{gesture_calinon_2010}{{8}{2010}{{Calinon et~al.}}{{Calinon, D'halluin, Sauser, Caldwell, and Billard}}}
\bibcite{Chambrier2014}{{9}{2014}{{Chambrier and Billard}}{{}}}
\bibcite{online_gpr_icra_2014}{{10}{2014}{{Cheng and Chen}}{{}}}
\bibcite{pomdp_peg_icra_2014}{{11}{2014}{{Cheng et~al.}}{{Cheng, Chen, Hao, and Li}}}
\bibcite{search_strategies_icra_2001}{{12}{2001}{{Chhatpar and Branicky}}{{}}}
\bibcite{p_search_surv_2011}{{13}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{EGW05}{{14}{2005}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{stable_FA_gordon_1995}{{15}{1995}{{Gordon}}{{}}}
\bibcite{rl_ac_surv_2012}{{16}{2012}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{learn_admittance_icra_1994}{{17}{1994}{{Gullapalli et~al.}}{{Gullapalli, Barto, and Grupen}}}
\bibcite{DRQ_AAAI_2015}{{18}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{learn_force_c_icirs_2011}{{19}{2011}{{Kalakrishnan et~al.}}{{Kalakrishnan, Righetti, Pastor, and Schaal}}}
\bibcite{Kronander2015}{{20}{2015}{{Kronander}}{{}}}
\bibcite{Lange_riedmiller_2010}{{21}{2010}{{Lange and Riedmiller}}{{}}}
\bibcite{Lauri2016}{{22}{2016}{{Lauri and Ritala}}{{}}}
\bibcite{peg_personal_icra_2010}{{23}{2010}{{Meeussen and et. al}}{{}}}
\bibcite{mnih-dqn-2015}{{24}{2015}{{Mnih and et. al}}{{}}}
\bibcite{trans_workpiece_icra_2013}{{25}{2013}{{Nemec et~al.}}{{Nemec, Abu-Dakka, Ridge, Ude, Jorgensen, Savarimuthu, Jouffroy, Petersen, and Kr\"uger}}}
\bibcite{fqi_nips_peter_2009}{{26}{2009{}}{{Neumann and Peters}}{{}}}
\bibcite{NIPS2008_3501}{{27}{2009{}}{{Neumann and Peters}}{{}}}
\bibcite{kernel_rl_ormoneit_2002}{{28}{2002}{{Ormoneit and Glynn}}{{}}}
\bibcite{NAC_2008}{{29}{2008{}}{{Peters and Schaal}}{{}}}
\bibcite{peter_nac_2008}{{30}{2008{}}{{Peters and Schaal}}{{}}}
\bibcite{PBVI}{{31}{2003}{{Pineau et~al.}}{{Pineau, Gordon, and Thrun}}}
\bibcite{Riedmiller2005}{{32}{2005}{{Riedmiller}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Distance taken to connect plug to socket once the socket is localised. (b) The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group BA first gave demonstrations on Socket B before giving demonstrations on Socket A. Group BA is better than Group AA at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (c) Both Groups AB and BB are similar in terms of the distance they took to insert the plug into the socket and as was the case for (b), the search policies travel less to accomplish the task. }}{15}{figure.15}}
\newlabel{fig:real_statistics}{{15}{15}{Distance taken to connect plug to socket once the socket is localised. (b) The human Group A are the set of teachers who first started with socket A. They had no previous training on another socket beforehand. Group BA first gave demonstrations on Socket B before giving demonstrations on Socket A. Group BA is better than Group AA at doing the task. This is most likely a training effect. However all policy search methods are far better at connecting the plug to the socket. (c) Both Groups AB and BB are similar in terms of the distance they took to insert the plug into the socket and as was the case for (b), the search policies travel less to accomplish the task}{figure.15}{}}
\bibcite{Schaal04learningmovement}{{33}{2004}{{Schaal et~al.}}{{Schaal, Peters, Nakanishi, and Ijspeert}}}
\bibcite{gmr_2004}{{34}{2004}{{Sung}}{{}}}
\bibcite{sutton1998reinforcement}{{35}{1998}{{Sutton and Barto}}{{}}}
\bibcite{toussain_2015}{{36}{2015}{{Vien and Toussaint}}{{}}}
\bibcite{RL_state_art_2012}{{37}{2012}{{Wiering and van Otterio}}{{}}}
\bibcite{fast_peg_pbd_icmc_2014}{{38}{2014}{{Yang and et~al.}}{{}}}
