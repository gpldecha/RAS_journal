\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{search_strategies_icra_2001}
\citation{online_gpr_icra_2014}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{1}{section.2}}
\newlabel{sec:related_work}{{2}{1}{Related work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Peg-in-hole}{1}{subsection.2.1}}
\citation{peg_personal_icra_2010}
\citation{search_strategies_icra_2001}
\citation{fast_peg_pbd_icmc_2014}
\citation{Schaal04learningmovement}
\citation{trans_workpiece_icra_2013,sol_pdg_pbd_2014}
\citation{Kronander2015}
\citation{learn_force_c_icirs_2011}
\citation{learn_admittance_icra_1994}
\citation{sutton98a}
\citation{rl_ac_surv_2012}
\citation{stable_FA_gordon_1995}
\citation{kernel_rl_ormoneit_2002}
\citation{fvi_uav_2010}
\citation{EGW05}
\citation{fqi_nips_peter_2009}
\citation{Riedmiller2005}
\citation{NAC_2008}
\citation{rl_gmm_2010}
\citation{Lange_riedmiller_2010}
\citation{mnih-dqn-2015}
\citation{DRQ_AAAI_2015}
\citation{approx_rl_overview_2011}
\citation{RL_state_art_2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Actor-Critic \& Fitted Reinforcement Learning}{2}{subsection.2.2}}
\citation{Bergman99recursivebayesian}
\citation{Kronander2015}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiment methods}{3}{section.3}}
\newlabel{sec:experiment_methods}{{3}{3}{Experiment methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Belief state}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Participants and experiment protocol}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Learning Actor and Critic}{3}{section.4}}
\newlabel{sec:learning-value-actor}{{4}{3}{Learning Actor and Critic}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The experimental setup. \textit  {Top-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit  {Top-right:} Dimensions of the the wall and socket. \textit  {Bottom:} Three different power sockets, only socket A and B are used for data collection, socket C is purely used for evaluating the generalisation of the learned policy.}}{4}{figure.1}}
\newlabel{fig:search_task_setup}{{1}{4}{The experimental setup. \textit {Top-left:} A participant (human teacher) is blindfolded and placed within the orange rectangular area always facing the wall. \textit {Top-right:} Dimensions of the the wall and socket. \textit {Bottom:} Three different power sockets, only socket A and B are used for data collection, socket C is purely used for evaluating the generalisation of the learned policy}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textit  {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit  {Right}: \textbf  {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf  {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one. }}{4}{figure.2}}
\newlabel{fig:PMF}{{2}{4}{\textit {Left:} Point Mass Filter (PMF) update of a particular human demonstration. (1) Initial uniform distribution spread over the starting region. Each grid cell represents a hypothetical position of the plug. The orientation is assumed to be known. (2) First contact, the distribution is spread across the surface of the wall. The red trace is the trajectory history. (3) motion noise increases the uncertainty. (4) The plug is in contact with a socket edge. \textit {Right}: \textbf {World model}: The plug is modelled by its three plug tips and the wall and sockets are fitted with bounding boxes. \textbf {Likelihood}: The plug enters in contact with the left edge of the socket. As a result, the value of the likelihood in all the regions, $x_t$, close the left edge take a value of one (red points) whilst the others have a value zero (blue points) and areas around the socket's central ring have a value of one}{figure.2}{}}
\citation{Atkeson97locallyweighted}
\citation{EGW05}
\citation{NIPS2008_3501,EGW05,Riedmiller2005}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded.}}{5}{figure.3}}
\newlabel{fig:experiment_design}{{3}{5}{Experiment protocol. The participants are divided in two groups of 5, Group A begins with socket A and after a short break repeats the task with socket B. The same logic holds for Group B. For each socket 15 executions of the task are recorded}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated.}}{5}{figure.4}}
\newlabel{fig:experiment_setup_data}{{4}{5}{\textit {Top}: Black points represent the starting position of the end-effector for all the demonstrations. Four trajectories are illustrated}{figure.4}{}}
\newlabel{eq:value_function}{{1}{5}{Learning Actor and Critic}{equation.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Actor \& Critic}{5}{subsection.4.1}}
\newlabel{eq:GMM}{{2}{5}{Actor \& Critic}{equation.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fitted Policy Iteration}{5}{subsection.4.2}}
\newlabel{sec:FPI}{{4.2}{5}{Fitted Policy Iteration}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Policy evaluation}{5}{section*.1}}
\newlabel{alg:fpe}{{1}{5}{Policy evaluation}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Fitted Policy Evaluation}}{5}{algocf.1}}
\citation{p_search_surv_2011}
\citation{peter_nac_2008}
\@writefile{toc}{\contentsline {paragraph}{Policy improvement}{6}{section*.2}}
\newlabel{eq:grad_log_cost}{{3}{6}{Policy improvement}{equation.4.3}{}}
\newlabel{eq:advantage_f}{{4}{6}{Policy improvement}{equation.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{2D example fitted policy iteration}{6}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Fitted policy evaluation \& improvement example. \textit  {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit  {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\boldsymbol  {\theta }}(x)$ is plotted in blue and trajectories generated by the policy $\mathbb  {E}\{\pi _{\boldsymbol  {\theta }}(\mathaccentV {dot}05F{x}|x)\}$ in orange. \textit  {Top-right} \textit  {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit  {Bottom-right} \textit  {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function. }}{6}{figure.5}}
\newlabel{fig:fpe_example}{{5}{6}{Fitted policy evaluation \& improvement example. \textit {Top-left:} The goal of the task is to reach the goal state. The first teacher (blue) demonstrates five trajectories which contours the obstacle in front of the goal. The second teacher (red) demonstrates 5 trajectories which initially deviate from the goal before passing between the two obstacles. \textit {Bottom-left:} The EM algorithm is used to fit a GMM to the teachers' original data. The marginal $\pi _{\Param }(\X )$ is plotted in blue and trajectories generated by the policy $\mathbb {E}\{\pi _{\Param }(\U |\X )\}$ in orange. \textit {Top-right} \textit {Policy Evaluation:}. Value function after fitted policy evaluation terminated, the reward function is binary, $r=1$ at the goal and zero otherwise, and a discount factor $\gamma = 0.99$ is used. \textit {Bottom-right} \textit {Policy Improvement:} the GMM is learned with the Q-EM algorithm in which each data point's weight proportional to the advantage function}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Belief state fitted policy evaluation}{6}{section*.4}}
\citation{gesture_calinon_2010}
\citation{gmr_2004}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textit  {Left}: LWR value function approximate $\mathaccentV {hat}05E{V}^{\pi }(\mathaccentV {hat}05E{x})$ for the most likely state $\mathaccentV {hat}05E{x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. \textit  {Middle-right}: Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$. }}{7}{figure.6}}
\newlabel{fig:ch4:Figure1}{{6}{7}{\textit {Left}: LWR value function approximate $\hat {V}^{\pi }(\hat {x})$ for the most likely state $\hat {x}$. The red plane is to help visualise where the value function is above and below the axis $z=0$. Only states with values above $0.25$ are plotted. The red arrow indicates the heading of the human teacher when performing the search task. The discount factor was $\gamma =0.99$ and the variance of the kernel variance of 1 [cm], which was set experimentally. \textit {Middle-right}: Best and worst trajectories. The red demonstrated trajectories are the best in terms of the amount of value function gain whilst the blue are the worst. The red arrow indicates the teacher's heading. The blue trajectories tend towards the sides of the wall as the initial starting position is on the boarders of the wall. The red trajectories are centred along the y-axis of socket and tend to move in a straight line towards the wall whilst aligning themselves with the axis $z=0$}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Control architecture}{7}{section.5}}
\newlabel{ch4:control_architecture}{{5}{7}{Control architecture}{section.5}{}}
\newlabel{eq:gmm_conditional}{{5}{7}{Control architecture}{equation.5.5}{}}
\newlabel{eq:alpha_eq}{{6}{7}{Control architecture}{equation.5.6}{}}
\newlabel{eq:alpha_expectation}{{7}{7}{Control architecture}{equation.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Robot Implementation}{7}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Q-EM and GMM policy vector fields. \textit  {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit  {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation.}}{8}{figure.7}}
\newlabel{fig:policy_vf}{{7}{8}{Q-EM and GMM policy vector fields. \textit {Top}: The GMM policy is conditioned on an entropy of $-10$ and $-5.2$. For the lowest entropy level, most of the probability mass is close to the socket area since this level corresponds to very little uncertainty; we are already localised. We can see that the policy converges to the socket area regardless of the location of the believed state. For an entropy of $-5.2$ we can see that the likelihood of the policy is present across wall. The vector field directs the end-effector to go towards the left or right edge of the wall. \textit {Bottom}: The entropy is marginalised out, the yellow vector field is of the Q-EM and orange of the GMM. The Q-EM vector field tends to be closer to a sink and there is less variation}{figure.7}{}}
\newlabel{eq:modulation}{{8}{8}{Robot Implementation}{equation.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{8}{section.6}}
\newlabel{sec:results}{{6}{8}{Results}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Control architecture. The PMF (belief) receives a measured velocity, $\mathaccentV {dot}05F{\mathaccentV {tilde}07E{x}}$, and a sensor measurement $\mathaccentV {tilde}07E{y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller.}}{8}{figure.8}}
\newlabel{fig:control_flow}{{8}{8}{Control architecture. The PMF (belief) receives a measured velocity, $\dot {\tilde {x}}$, and a sensor measurement $\tilde {y}$ and is updated via Bayes rule. The belief is compressed and used by both the GMM policy and the proportional speed controller}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Distance taken to reach the socket's edge}{9}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Importance of data}{9}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Two simulated search experiments. \textbf  {Experiment 1:} \textit  {Top-left}: Three start positions are considered: \textit  {Left}, \textit  {Center} and \textit  {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. \textit  {Bottom-left}: Trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textit  {Bottom-right:} Distribution of first contact point giving the center initial starting condition. \textit  {Top-right}: Distribution of distance travelled until the socket's edge was localised. The Q-EM policy is always the best. \textbf  {Experiment 2}: \textit  {Left}: Distribution of the visited regions during the search for the socket's edge. The Q-EM policy's distribution is more centred along the axis $z=0$. \textit  {Right:} Time taken to find the socket, the search algorithms are better than the humans with the exception of group BA.}}{10}{figure.9}}
\newlabel{fig:experiment12}{{9}{10}{Two simulated search experiments. \textbf {Experiment 1:} \textit {Top-left}: Three start positions are considered: \textit {Left}, \textit {Center} and \textit {Right} in which the triangles depict true position of the end-effector. The red cube illustrates the extent of the uncertainty. \textit {Bottom-left}: Trajectories of both the GMM (orange) and Q-EM (yellow) policies. For each start condition a total of 25 searches were performed for each search policy. \textit {Bottom-right:} Distribution of first contact point giving the center initial starting condition. \textit {Top-right}: Distribution of distance travelled until the socket's edge was localised. The Q-EM policy is always the best. \textbf {Experiment 2}: \textit {Left}: Distribution of the visited regions during the search for the socket's edge. The Q-EM policy's distribution is more centred along the axis $z=0$. \textit {Right:} Time taken to find the socket, the search algorithms are better than the humans with the exception of group BA}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy.}}{11}{figure.11}}
\newlabel{fig:experiment3_stats}{{11}{11}{Results of a GMM and Q-EM policy under the same test conditions as Experiment 1. The Q-EM policy nearly always does much better than the GMM policy}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Generalisation}{11}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Experiment 3} \textit  {Top-left}: Demonstrations of teacher \# 5. \textit  {Bottom-left} Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted. \textit  {Middle-column}: Most likely state parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. \textit  {Right-column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge.}}{12}{figure.10}}
\newlabel{fig:experiment3}{{10}{12}{\textbf {Experiment 3} \textit {Top-left}: Demonstrations of teacher \# 5. \textit {Bottom-left} Value function learned from the 15 demonstrations of teacher \#5. The value of the most likely state is plotted. \textit {Middle-column}: Most likely state parameters of the GMM and Q-EM learned from the demonstrations of teacher \#5. \textit {Right-column}: Rollouts of the policies learned from teacher \#5. We can see that trajectories from the GMM policy have not really encoded a specific search patter, whilst the Q-EM policy gives many more consistent trajectories which replicate to some extent the pattern of making a jump (no contact with the wall) from the top right corner to the socket's edge}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Experiment 4} Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit  {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1. }}{12}{figure.12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{12}{figure.12}}
\newlabel{fig:experiment4}{{12}{12}{\textbf {Experiment 4} Evaluation of generalisation. The socket is located in at the top right corner of the wall. We consider a \textit {Fixed} starting location for both the true and believed location of the end-effector. The red square depicts the extent of the initial uncertainty, which is uniform. (b) Distance taken to reach the socket's edge. For the Fixed setup (see (a) for the initial condition), both the Q-EM and GMM significantly outperform the Greedy. The other three conditions are the same as for Experiment 1}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Distance taken to connect the plug to the socket}{13}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket.}}{13}{figure.14}}
\newlabel{fig:real_statistics2}{{14}{13}{Distance taken (measured from point of contact of plug with socket edge) to connect the plug to the socket}{figure.14}{}}
\citation{Chambrier2014}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textit  {Left}: 25 search trajectories for each of the three search policies for socket A. \textit  {Right} KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. \textit  {Bottom}: Socket C, same initial condition. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection.}}{14}{figure.13}}
\newlabel{fig:real_pictures}{{13}{14}{\textit {Left}: 25 search trajectories for each of the three search policies for socket A. \textit {Right} KUKA LWR4 equipped with a holder mounted with a ATI 6-axis force-torque sensor. (a) The robot's end-effector starts to the right of socket A. The second row shows screen captures taken of ROS Rviz data visualiser in which we see the Point Mass Filter (red particles) and a yellow arrow indicating the direction given by the policy. In this particular run, the plug remained in contact with the ring of the socket until the top was reached before making a connection. \textit {Bottom}: Socket C, same initial condition. The policy leads the plug down to the bottom corner of the socket before going the center of the top edge, localising itself, and then making a connection}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion \& Conclusion}{14}{section.7}}
\newlabel{sec:conclusion}{{7}{14}{Discussion \& Conclusion}{section.7}{}}
\bibstyle{elsarticle-harv}
\bibdata{bib/peg_hole.bib,bib/pomdp.bib,bib/RL.bib,ch3-citations.bib,bib/DT.bib,bib/MLMF.bib,bib/spatial_navigation.bib}
\bibcite{sol_pdg_pbd_2014}{{1}{2014}{{Abu-Dakka et~al.}}{{Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger, and Ude}}}
\bibcite{rl_gmm_2010}{{2}{2010}{{Agostini and Celaya}}{{}}}
\bibcite{Atkeson97locallyweighted}{{3}{1997}{{Atkeson et~al.}}{{Atkeson, Moore, and Schaal}}}
\bibcite{Bergman99recursivebayesian}{{4}{1999}{{Bergman and Bergman}}{{}}}
\bibcite{fvi_uav_2010}{{5}{2010}{{Bou-Ammar et~al.}}{{Bou-Ammar, Voos, and Ertel}}}
\bibcite{approx_rl_overview_2011}{{6}{2011}{{Busoniu et~al.}}{{Busoniu, Ernst, Schutter, and Babuska}}}
\bibcite{gesture_calinon_2010}{{7}{2010}{{Calinon et~al.}}{{Calinon, D'halluin, Sauser, Caldwell, and Billard}}}
\bibcite{Chambrier2014}{{8}{2014}{{Chambrier and Billard}}{{}}}
\bibcite{online_gpr_icra_2014}{{9}{2014}{{Cheng and Chen}}{{}}}
\bibcite{search_strategies_icra_2001}{{10}{2001}{{Chhatpar and Branicky}}{{}}}
\bibcite{p_search_surv_2011}{{11}{2011}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{EGW05}{{12}{2005}{{Ernst et~al.}}{{Ernst, Geurts, and Wehenkel}}}
\bibcite{stable_FA_gordon_1995}{{13}{1995}{{Gordon}}{{}}}
\bibcite{rl_ac_surv_2012}{{14}{2012}{{Grondman et~al.}}{{Grondman, Busoniu, Lopes, and Babuska}}}
\bibcite{learn_admittance_icra_1994}{{15}{1994}{{Gullapalli et~al.}}{{Gullapalli, Barto, and Grupen}}}
\bibcite{DRQ_AAAI_2015}{{16}{2015}{{Hausknecht and Stone}}{{}}}
\bibcite{learn_force_c_icirs_2011}{{17}{2011}{{Kalakrishnan et~al.}}{{Kalakrishnan, Righetti, Pastor, and Schaal}}}
\bibcite{Lange_riedmiller_2010}{{18}{2010}{{Lange and Riedmiller}}{{}}}
\bibcite{peg_personal_icra_2010}{{19}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{mnih-dqn-2015}{{20}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}}}
\bibcite{trans_workpiece_icra_2013}{{21}{2013}{{Nemec et~al.}}{{Nemec, Abu-Dakka, Ridge, Ude, Jorgensen, Savarimuthu, Jouffroy, Petersen, and Kr\"uger}}}
\bibcite{fqi_nips_peter_2009}{{22}{2009{}}{{Neumann and Peters}}{{}}}
\bibcite{NIPS2008_3501}{{23}{2009{}}{{Neumann and Peters}}{{}}}
\bibcite{kernel_rl_ormoneit_2002}{{24}{2002}{{Ormoneit and Glynn}}{{}}}
\bibcite{NAC_2008}{{25}{2008{}}{{Peters and Schaal}}{{}}}
\bibcite{peter_nac_2008}{{26}{2008{}}{{Peters and Schaal}}{{}}}
\bibcite{Riedmiller2005}{{27}{2005}{{Riedmiller}}{{}}}
\bibcite{Schaal04learningmovement}{{28}{2004}{{Schaal et~al.}}{{Schaal, Peters, Nakanishi, and Ijspeert}}}
\bibcite{gmr_2004}{{29}{2004}{{Sung}}{{}}}
\bibcite{sutton98a}{{30}{1998}{{Sutton and Barto}}{{}}}
\bibcite{RL_state_art_2012}{{31}{2012}{{Wiering and van Otterio}}{{}}}
\bibcite{fast_peg_pbd_icmc_2014}{{32}{2014}{{Yang et~al.}}{{Yang, Lin, Song, Nemec, Ude, Buch, Kr√ºger, and Savarimuthu}}}
