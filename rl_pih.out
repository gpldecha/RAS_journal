\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Background}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{POMDP}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Fitted Reinforcement Learning}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Monte-Carlo Expectation-Maximisation policy search}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.4}{Peg-in-hole}{section.2}% 6
\BOOKMARK [1][-]{section.3}{Methodology}{}% 7
\BOOKMARK [2][-]{subsection.3.1}{PiH-search experiment}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.2}{Data collection}{section.3}% 9
\BOOKMARK [1][-]{section.4}{Learning}{}% 10
\BOOKMARK [2][-]{subsection.4.1}{Fitted Policy Iteration}{section.4}% 11
\BOOKMARK [1][-]{section.5}{Control architecture}{}% 12
\BOOKMARK [2][-]{subsection.5.1}{Robot Implementation}{section.5}% 13
\BOOKMARK [1][-]{section.6}{Results}{}% 14
\BOOKMARK [2][-]{subsection.6.1}{Distance taken to reach the socket's edge}{section.6}% 15
\BOOKMARK [2][-]{subsection.6.2}{Importance of data}{section.6}% 16
\BOOKMARK [2][-]{subsection.6.3}{Generalisation}{section.6}% 17
\BOOKMARK [2][-]{subsection.6.4}{Distance taken to connect the plug to the socket}{section.6}% 18
\BOOKMARK [1][-]{section.7}{Discussion \046 Conclusion}{}% 19
