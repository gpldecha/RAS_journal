\begin{thebibliography}{56}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abu-Dakka et~al.(2014)Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger,
  and Ude}]{sol_pdg_pbd_2014}
Abu-Dakka, F., Nemec, B., Kramberger, A., Buch, A.~G., Kr{\"u}ger, N., Ude, A.,
  2014. Solving peg-in-hole tasks by human demonstration and exception
  strategies. Industrial Robot 41~(6), 575--584.
\newline\urlprefix\url{http://dx.doi.org/10.1108/IR-07-2014-0363}

\bibitem[{Agostini and Celaya(2010)}]{rl_gmm_2010}
Agostini, A., Celaya, E., July 2010. Reinforcement learning with a gaussian
  mixture model. In: International Joint Conference on Neural Networks (IJCNN).
  pp. 1--8.
\newline\urlprefix\url{http://dx.doi.org/10.1109/IJCNN.2010.5596306}

\bibitem[{Atkeson et~al.(1997)Atkeson, Moore, and
  Schaal}]{Atkeson97locallyweighted}
Atkeson, C., Moore, A., Schaal, S., 1997. Locally weighted learning. Artificial
  Inteligence, 11--73.
\newline\urlprefix\url{http://dx.doi.org/10.1023/A:1006559212014}

\bibitem[{Baird(1995)}]{Baird95}
Baird, L., 1995. Residual algorithms: Reinforcement learning with function
  approximation. In: In Proceedings of the Twelfth International Conference on
  Machine Learning. Morgan Kaufmann, pp. 30--37.

\bibitem[{Baxter and Bartlett(2000)}]{Baxter_GPOMDP_2000}
Baxter, J., Bartlett, P., 2000. Reinforcement learning in pomdp's via direct
  gradient ascent. In: International Conference on Machine Learning. Vol.~17.
  Morgan Kaufmann, pp. 41--48.

\bibitem[{Bergman and Bergman(1999)}]{Bergman99recursivebayesian}
Bergman, N., Bergman, N., 1999. Recursive bayesian estimation: Navigation and
  tracking applications. thesis no 579. Tech. rep., Link\"oping University,
  Link\"oping Studies in Science and Technology. Doctoral dissertation.

\bibitem[{Bilmes(1997)}]{Bilmes97agentle}
Bilmes, J., 1997. A gentle tutorial on the em algorithm and its application to
  parameter estimation for gaussian mixture and hidden markov models. Tech.
  rep.

\bibitem[{Bishop(2006)}]{Bishop_2006}
Bishop, C.~M., 2006. Pattern Recognition and Machine Learning. Springer.

\bibitem[{Bou-Ammar et~al.(2010)Bou-Ammar, Voos, and Ertel}]{fvi_uav_2010}
Bou-Ammar, H., Voos, H., Ertel, W., Sept 2010. Controller design for quadrotor
  uavs using reinforcement learning. In: International Conference on Control
  Applications. pp. 2130--2135.
\newline\urlprefix\url{http://dx.doi.org/10.1109/CCA.2010.5611206}

\bibitem[{Boyan and Moore(1995)}]{Boyan95generalizationin}
Boyan, J., Moore, A., 1995. Generalization in reinforcement learning: Safely
  approximating the value function. In: Advances in Neural Information
  Processing Systems (NIPS). Vol.~7. pp. 369--376.

\bibitem[{Brooks et~al.(2006)Brooks, Makarenko, Williams, and
  Durrant-Whyte}]{PPOMDP_2006}
Brooks, A., Makarenko, A., Williams, S., Durrant-Whyte, H., 2006. {Parametric
  {\{}POMDPs{\}} for planning in continuous state spaces}. Robotics and
  Autonomous Systems 54~(11), 887--897.
\newline\urlprefix\url{http://www.sciencedirect.com/science/article/pii/S0921889006000960}

\bibitem[{Busoniu et~al.(2011)Busoniu, Ernst, {de Schutter}, and
  Babuska}]{approx_rl_overview_2011}
Busoniu, L., Ernst, D., {de Schutter}, B., Babuska, R., April 2011. Approximate
  reinforcement learning: An overview. In: Symposium on Adaptive Dynamic
  Programming and Reinforcement Learning (ADPRL). pp. 1--8.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ADPRL.2011.5967353}

\bibitem[{Calinon et~al.(2010)Calinon, D'halluin, Sauser, Caldwell, and
  Billard}]{gesture_calinon_2010}
Calinon, S., D'halluin, F., Sauser, E., Caldwell, D., Billard, A., jun 2010.
  Learning and reproduction of gestures by imitation. IEEE Robotics Automation
  Magazine 17~(2), 44--54.
\newline\urlprefix\url{http://dx.doi.org/10.1109/MRA.2010.936947}

\bibitem[{Calinon et~al.(2013)Calinon, Kormushev, and
  Caldwell}]{Calinon2013369}
Calinon, S., Kormushev, P., Caldwell, D.~G., 2013. {Compliant skills
  acquisition and multi-optima policy search with EM-based reinforcement
  learning}. Robotics and Autonomous Systems 61~(4), 369--379.
\newline\urlprefix\url{http://www.sciencedirect.com/science/article/pii/S0921889012001662}

\bibitem[{Chambrier and Billard(2014)}]{Chambrier2014}
Chambrier, G.~d., Billard, A., 2014. Learning search policies from humans in a
  partially observable context. Journal of Robotics and Biomimetics 1~(1),
  1--16.

\bibitem[{Cheng and Chen(2014)}]{online_gpr_icra_2014}
Cheng, H., Chen, H., may 2014. Online parameter optimization in robotic force
  controlled assembly processes. In: International Conference on Robotics and
  Automation (ICRA). pp. 3465--3470.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ICRA.2014.6907358}

\bibitem[{Cheng et~al.(2014)Cheng, Chen, Hao, and Li}]{pomdp_peg_icra_2014}
Cheng, H., Chen, H., Hao, L., Li, W., May 2014. Robot learning based on partial
  observable markov decision process in unstructured environment. In:
  International Conference on Robotics and Automation (ICRA). pp. 4399--4404.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ICRA.2014.6907500}

\bibitem[{Deisenroth et~al.(2013)Deisenroth, Neumann, and
  Peters}]{p_search_surv_2011}
Deisenroth, M.~P., Neumann, G., Peters, J., 2013. A survey on policy search for
  robotics. Foundations and Trends in Robotics 2~(1-2), 1--142.
\newline\urlprefix\url{http://dx.doi.org/10.1561/2300000021}

\bibitem[{Du et~al.(2010)Du, Hsu, Kurniawati, Lee, Ong, and
  Png}]{POMDP_approach_2010}
Du, Y., Hsu, D., Kurniawati, H., Lee, W., Ong, S., Png, S., 2010. A pomdp
  approach to robot motion planning under uncertainty. In: International
  Conference on Automated Planning and Scheduling, Workshop on Solving
  Real-World POMDP Problems.

\bibitem[{Ernst et~al.(2005)Ernst, Geurts, and Wehenkel}]{EGW05}
Ernst, D., Geurts, P., Wehenkel, L., April 2005. Tree-based batch mode
  reinforcement learning. Journal of Machine Learning Research 6, 503--556.

\bibitem[{Gordon(1995)}]{stable_FA_gordon_1995}
Gordon, G., 1995. Stable function approximation in dynamic programming. In:
  International Conference on Machine Learning (ICML). Carnegie Mellon
  University.
\newline\urlprefix\url{"http://www.cs.cmu.edu/~ggordon/ml95-stable-dp.ps.gz"}

\bibitem[{Grondman et~al.(2012)Grondman, Busoniu, Lopes, and
  Babuska}]{rl_ac_surv_2012}
Grondman, I., Busoniu, L., Lopes, G., Babuska, R., Nov 2012. A survey of
  actor-critic reinforcement learning: Standard and natural policy gradients.
  Transactions on Systems, Man, and Cybernetics, Part C (Applications and
  Reviews) 42~(6), 1291--1307.
\newline\urlprefix\url{http://dx.doi.org/10.1109/TSMCC.2012.2218595}

\bibitem[{Gullapalli et~al.(1994)Gullapalli, Barto, and
  Grupen}]{learn_admittance_icra_1994}
Gullapalli, V., Barto, A., Grupen, R., may 1994. Learning admittance mappings
  for force-guided assembly. In: International Conference on Robotics and
  Automation (ICRA). pp. 2633--2638.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ROBOT.1994.351117}

\bibitem[{Hausknecht and Stone(2015)}]{DRQ_AAAI_2015}
Hausknecht, M., Stone, P., 2015. Deep recurrent q-learning for partially
  observable mdps. Association for the Advancement of Artificial Intelligence
  (AAAI).
\newline\urlprefix\url{https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11673}

\bibitem[{Hauskrecht(2000)}]{Milos_POMDP_2000}
Hauskrecht, M., Aug 2000. Value-function approximations for partially
  observable markov decision processes. Journal of Artificial Intelligence
  Research 13~(1), 33--94.
\newline\urlprefix\url{http://dl.acm.org/citation.cfm?id=1622262.1622264}

\bibitem[{Jodogne et~al.(2006)Jodogne, Briquet, and Piater}]{Jodogne2006}
Jodogne, S., Briquet, C., Piater, J.~H., 2006. {Approximate Policy Iteration
  for Closed-Loop Learning of Visual Tasks}. Springer Berlin Heidelberg, pp.
  210--221.
\newline\urlprefix\url{http://dx.doi.org/10.1007/11871842{\_}23}

\bibitem[{Kalakrishnan et~al.(2011)Kalakrishnan, Righetti, Pastor, and
  Schaal}]{learn_force_c_icirs_2011}
Kalakrishnan, M., Righetti, L., Pastor, P., Schaal, S., Sept 2011. Learning
  force control policies for compliant manipulation. In: International
  Conference on Intelligent Robots and Systems (ICRA). pp. 4639--4644.
\newline\urlprefix\url{http://dx.doi.org/10.1109/IROS.2011.6095096}

\bibitem[{Kormushev et~al.(2010)Kormushev, Calinon, Saegusa, and
  Metta}]{Kormushev2010Humanoids}
Kormushev, P., Calinon, S., Saegusa, R., Metta, G., December 2010. Learning the
  skill of archery by a humanoid robot {iCub}. In: International Conference on
  Humanoid Robots ({H}umanoids). Nashville, USA, pp. 417--423.
\newline\urlprefix\url{http://kormushev.com/papers/Kormushev_Humanoids-2010.pdf}

\bibitem[{Kronander(2015)}]{Kronander2015}
Kronander, K., 2015. Control and learning of compliant manipulation skills.

\bibitem[{Lagoudakis and Parr(2003)}]{LSPI_2003}
Lagoudakis, M.~G., Parr, R., dec 2003. Least-squares policy iteration. Journal
  of Machine Learning Research 4, 1107--1149.
\newline\urlprefix\url{http://dl.acm.org/citation.cfm?id=945365.964290}

\bibitem[{Lange and Riedmiller(2010)}]{Lange_riedmiller_2010}
Lange, S., Riedmiller, M., July 2010. Deep auto-encoder neural networks in
  reinforcement learning. In: International Joint Conference on Neural Networks
  (IJCNN). pp. 1--8.

\bibitem[{Lauri and Ritala(2016)}]{Lauri2016}
Lauri, M., Ritala, R., 2016. Planning for robotic exploration based on forward
  simulation. Robotics and Autonomous Systems.

\bibitem[{Meeussen et~al.(2010)Meeussen, Wise, Glaser, Chitta, and et.
  al}]{peg_personal_icra_2010}
Meeussen, W., Wise, M., Glaser, S., Chitta, S., et. al, May 2010. Autonomous
  door opening and plugging in with a personal robot. In: International
  Conference on Robotics and Automation (ICRA). pp. 729--736.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ROBOT.2010.5509556}

\bibitem[{Melo and Lopes(2008)}]{Melo2008}
Melo, F.~S., Lopes, M., 2008. {Fitted Natural Actor-Critic: A New Algorithm for
  Continuous State-Action MDPs}. Springer Berlin Heidelberg, pp. 66--81.
\newline\urlprefix\url{http://dx.doi.org/10.1007/978-3-540-87481-2{\_}5}

\bibitem[{Mnih(2015)}]{mnih-dqn-2015}
Mnih, V., 02 2015. Human-level control through deep reinforcement learning.
  Nature 518~(7540), 529--533.
\newline\urlprefix\url{http://dx.doi.org/10.1038/nature14236}

\bibitem[{Nemec et~al.(2013)Nemec, Abu-Dakka, Ridge, and et.
  al}]{trans_workpiece_icra_2013}
Nemec, B., Abu-Dakka, F., Ridge, B., et. al, Nov 2013. Transfer of assembly
  operations to new workpiece poses by adaptation to the desired force profile.
  In: International Conference on Advanced Robotics (ICAR). pp. 1--7.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ICAR.2013.6766568}

\bibitem[{Neumann and Peters(2009{\natexlab{a}})}]{fqi_nips_peter_2009}
Neumann, G., Peters, J., Jun 2009{\natexlab{a}}. Fitted q-iteration by
  advantage weighted regression. In: Advances in Neural Information Processing
  Systems (NIPS). Vol.~21. pp. 1177--1184.

\bibitem[{Neumann and Peters(2009{\natexlab{b}})}]{NIPS2008_3501}
Neumann, G., Peters, J.~R., 2009{\natexlab{b}}. Fitted q-iteration by advantage
  weighted regression. In: Koller, D., Schuurmans, D., Bengio, Y., Bottou, L.
  (Eds.), Advances in Neural Information Processing Systems (NIPS). Vol.~21.
  Curran Associates, Inc., pp. 1177--1184.

\bibitem[{Ormoneit and Glynn(2002)}]{kernel_rl_ormoneit_2002}
Ormoneit, D., Glynn, P., Oct 2002. Kernel-based reinforcement learning in
  average-cost problems. Transactions on Automatic Control 47~(10), 1624--1636.
\newline\urlprefix\url{http://dx.doi.org/10.1109/TAC.2002.803530}

\bibitem[{Peters and Schaal(2008{\natexlab{a}})}]{NAC_2008}
Peters, J., Schaal, S., 2008{\natexlab{a}}. Natural actor-critic. European
  Symposium on Artificial Neural Networks 71~(7-9), 1180--1190.
\newline\urlprefix\url{http://dx.doi.org/10.1016/j.neucom.2007.11.026}

\bibitem[{Peters and Schaal(2008{\natexlab{b}})}]{peter_nac_2008}
Peters, J., Schaal, S., mar 2008{\natexlab{b}}. Natural actor-critic.
  Neurocomputing 71~(7-9), 1180--1190.
\newline\urlprefix\url{http://dx.doi.org/10.1016/j.neucom.2007.11.026}

\bibitem[{Pineau et~al.(2003)Pineau, Gordon, and Thrun}]{PBVI_2003}
Pineau, J., Gordon, G., Thrun, S., Aug 2003. Point-based value iteration: an
  anytime algorithm for pomdps. In: International Joint Conference on
  Artificial Intelligence (IJCAI). pp. 1025--1030.
\newline\urlprefix\url{http://dl.acm.org/citation.cfm?id=1630659.1630806}

\bibitem[{Porta et~al.(2006)Porta, Vlassis, Spaan, and Poupart}]{cPBVI_2006}
Porta, J., Vlassis, N., Spaan, M., Poupart, P., Dec 2006. Point-based value
  iteration for continuous pomdps. J. Mach. Learn. Res. 7, 2329--2367.
\newline\urlprefix\url{http://dl.acm.org/citation.cfm?id=1248547.1248630}

\bibitem[{Riedmiller(2005)}]{Riedmiller2005}
Riedmiller, M., 2005. Neural Fitted Q Iteration - First Experiences with a Data
  Efficient Neural Reinforcement Learning Method. Vol.~16. Springer Berlin
  Heidelberg, pp. 317--328.
\newline\urlprefix\url{http://dx.doi.org/10.1007/11564096_32}

\bibitem[{Ross et~al.(2008)Ross, Pineau, Paquet, and
  Chaib-draa}]{Ross08onlineplanning}
Ross, S., Pineau, J., Paquet, S., Chaib-draa, B., Jul 2008. Online planning
  algorithms for pomdps. Journal Artifcial Intelligence Research 32~(1),
  663--704.
\newline\urlprefix\url{http://dl.acm.org/citation.cfm?id=1622673.1622690}

\bibitem[{Siddharth and Branicky(2001)}]{search_strategies_icra_2001}
Siddharth, C., Branicky, M., 2001. Search strategies for peg-in-hole assemblies
  with position uncertainty. In: International Conference on Intelligent Robots
  and Systems (ICRA). Vol.~3. pp. 1465--1470.
\newline\urlprefix\url{http://dx.doi.org/10.1109/IROS.2001.977187}

\bibitem[{Smallwood and Sondik(1973)}]{Sondik_1973}
Smallwood, R., Sondik, E., Oct 1973. The optimal control of partially
  observable markov processes over a finite horizon. Journal of Operational
  Research 21~(5), 1071--1088.
\newline\urlprefix\url{http://dx.doi.org/10.1287/opre.21.5.1071}

\bibitem[{Sung(2004)}]{gmr_2004}
Sung, H.~G., 2004. Gaussian mixture regression and classification. Ph.D.
  thesis, Rice University.

\bibitem[{Sutton and Barto(1998)}]{sutton1998reinforcement}
Sutton, R.~S., Barto, A., 1998. Reinforcement Learning: An Introduction. MIT
  Press, Cambridge, MA.

\bibitem[{Sutton et~al.(2000)Sutton, Mcallester, Singh, and
  Mansour}]{Sutton00policygradient}
Sutton, R.~S., Mcallester, D., Singh, S., Mansour, Y., 2000. Policy gradient
  methods for reinforcement learning with function approximation. In: Neural
  Information Processing Systems (NIPS). Vol.~12. MIT Press, pp. 1057--1063.

\bibitem[{Thrun(2000)}]{Thrun_1999}
Thrun, S., 2000. {Monte Carlo POMDPs}. In: Advances in Neural Information
  Processing Systems (NIPS). Vol.~12. MIT Press, pp. 1064--1070.
\newline\urlprefix\url{http://papers.nips.cc/paper/1772-monte-carlo-pomdps.pdf}

\bibitem[{Thrun et~al.(2005)Thrun, Burgard, and Fox}]{Thrun_Burgard_Fox_2005}
Thrun, S., Burgard, W., Fox, D., 2005. Probabilistic Robotics. The MIT Press.

\bibitem[{Veiga et~al.(2014)Veiga, Spaan, and Lima}]{Veiga14aaai}
Veiga, T., Spaan, M., Lima, P., 2014. Point-based {POMDP} solving with factored
  value function approximation. In: Conference on Artificial Intelligence
  (AAAI). Vol.~28. pp. 2512--2518.

\bibitem[{Vien and Toussaint(2015)}]{toussain_2015}
Vien, N., Toussaint, M., Sept 2015. Pomdp manipulation via trajectory
  optimization. In: International Conference on Intelligent Robots and Systems
  (IROS). pp. 242--249.
\newline\urlprefix\url{http://dx.doi.org/10.1109/IROS.2015.7353381}

\bibitem[{Wiering and van Otterio(2012)}]{RL_state_art_2012}
Wiering, M., van Otterio, M., 2012. Reinforcement Learning State-of-the-Art.
  Springer-Verlag Berlin Heidelberg.

\bibitem[{Yang et~al.(2014)Yang, Lin, Song, Nemec, Ude, Buch, Kruger, and
  Savarimuthu}]{fast_peg_pbd_icmc_2014}
Yang, Y., Lin, L., Song, Y., Nemec, B., Ude, A., Buch, A., Kruger, N.,
  Savarimuthu, T., 2014. Fast programming of peg-in-hole actions by human
  demonstration. pp. 990--995.
\newline\urlprefix\url{http://dx.doi.org/10.1109/ICMC.2014.7231702}

\end{thebibliography}
