\begin{thebibliography}{32}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abu-Dakka et~al.(2014)Abu-Dakka, Nemec, Kramberger, Buch, Kr{\"u}ger,
  and Ude}]{sol_pdg_pbd_2014}
Abu-Dakka, F., Nemec, B., Kramberger, A., Buch, A.~G., Kr{\"u}ger, N., Ude, A.,
  2014. Solving peg-in-hole tasks by human demonstration and exception
  strategies. Industrial Robot 41~(6), 575--584.

\bibitem[{Agostini and Celaya(2010)}]{rl_gmm_2010}
Agostini, A., Celaya, E., July 2010. Reinforcement learning with a gaussian
  mixture model. In: The 2010 International Joint Conference on Neural Networks
  (IJCNN). pp. 1--8.

\bibitem[{Atkeson et~al.(1997)Atkeson, Moore, and
  Schaal}]{Atkeson97locallyweighted}
Atkeson, C.~G., Moore, A.~W., Schaal, S., 1997. Locally weighted learning.
  ARTIFICIAL INTELLIGENCE REVIEW, 11--73.

\bibitem[{Bergman and Bergman(1999)}]{Bergman99recursivebayesian}
Bergman, N., Bergman, C.~N., 1999. Recursive bayesian estimation: Navigation
  and tracking applications. thesis no 579. Tech. rep., Link\"oping University,
  Link\"oping Studies in Science and Technology. Doctoral dissertation.

\bibitem[{Bou-Ammar et~al.(2010)Bou-Ammar, Voos, and Ertel}]{fvi_uav_2010}
Bou-Ammar, H., Voos, H., Ertel, W., Sept 2010. Controller design for quadrotor
  uavs using reinforcement learning. In: 2010 IEEE International Conference on
  Control Applications. pp. 2130--2135.

\bibitem[{Busoniu et~al.(2011)Busoniu, Ernst, Schutter, and
  Babuska}]{approx_rl_overview_2011}
Busoniu, L., Ernst, D., Schutter, B.~D., Babuska, R., April 2011. Approximate
  reinforcement learning: An overview. In: 2011 IEEE Symposium on Adaptive
  Dynamic Programming and Reinforcement Learning (ADPRL). pp. 1--8.

\bibitem[{Calinon et~al.(2010)Calinon, D'halluin, Sauser, Caldwell, and
  Billard}]{gesture_calinon_2010}
Calinon, S., D'halluin, F., Sauser, E.~L., Caldwell, D.~G., Billard, A.~G.,
  June 2010. Learning and reproduction of gestures by imitation. IEEE Robotics
  Automation Magazine 17~(2), 44--54.

\bibitem[{Chambrier and Billard(2014)}]{Chambrier2014}
Chambrier, G.~d., Billard, A., 2014. Learning search policies from humans in a
  partially observable context. Robotics and Biomimetics 1~(1), 1--16.
\newline\urlprefix\url{http://dx.doi.org/10.1186/s40638-014-0008-1}

\bibitem[{Cheng and Chen(2014)}]{online_gpr_icra_2014}
Cheng, H., Chen, H., May 2014. Online parameter optimization in robotic force
  controlled assembly processes. In: 2014 IEEE International Conference on
  Robotics and Automation (ICRA). pp. 3465--3470.

\bibitem[{Chhatpar and Branicky(2001)}]{search_strategies_icra_2001}
Chhatpar, S.~R., Branicky, M.~S., 2001. Search strategies for peg-in-hole
  assemblies with position uncertainty. In: Intelligent Robots and Systems,
  2001. Proceedings. 2001 IEEE/RSJ International Conference on. Vol.~3. pp.
  1465--1470 vol.3.

\bibitem[{Deisenroth et~al.(2011)Deisenroth, Neumann, and
  Peters}]{p_search_surv_2011}
Deisenroth, M.~P., Neumann, G., Peters, J., 2011. A survey on policy search for
  robotics. Foundations and Trends in Robotics 2~(1-2), 1--142.
\newline\urlprefix\url{http://dx.doi.org/10.1561/2300000021}

\bibitem[{Ernst et~al.(2005)Ernst, Geurts, and Wehenkel}]{EGW05}
Ernst, D., Geurts, P., Wehenkel, L., April 2005. Tree-based batch mode
  reinforcement learning. Journal of Machine Learning Research 6, 503--556.

\bibitem[{Gordon(1995)}]{stable_FA_gordon_1995}
Gordon, G.~J., 1995. Stable function approximation in dynamic programming. In:
  Proceedings of the Twelfth International Conference on Machine Learning
  (ICML). Carnegie Mellon University.
\newline\urlprefix\url{"http://www.cs.cmu.edu/~ggordon/ml95-stable-dp.ps.gz"}

\bibitem[{Grondman et~al.(2012)Grondman, Busoniu, Lopes, and
  Babuska}]{rl_ac_surv_2012}
Grondman, I., Busoniu, L., Lopes, G. A.~D., Babuska, R., Nov 2012. A survey of
  actor-critic reinforcement learning: Standard and natural policy gradients.
  IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
  Reviews) 42~(6), 1291--1307.

\bibitem[{Gullapalli et~al.(1994)Gullapalli, Barto, and
  Grupen}]{learn_admittance_icra_1994}
Gullapalli, V., Barto, A.~G., Grupen, R.~A., 1994. Learning admittance mappings
  for force-guided assembly. In: Proceedings of the 1994 International
  Conference on Robotics and Automation, San Diego, CA, USA, May 1994. pp.
  2633--2638.

\bibitem[{Hausknecht and Stone(2015)}]{DRQ_AAAI_2015}
Hausknecht, M., Stone, P., 2015. Deep recurrent q-learning for partially
  observable mdps.
\newline\urlprefix\url{https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11673}

\bibitem[{Kalakrishnan et~al.(2011)Kalakrishnan, Righetti, Pastor, and
  Schaal}]{learn_force_c_icirs_2011}
Kalakrishnan, M., Righetti, L., Pastor, P., Schaal, S., Sept 2011. Learning
  force control policies for compliant manipulation. In: 2011 IEEE/RSJ
  International Conference on Intelligent Robots and Systems. pp. 4639--4644.

\bibitem[{Lange and Riedmiller(2010)}]{Lange_riedmiller_2010}
Lange, S., Riedmiller, M., July 2010. Deep auto-encoder neural networks in
  reinforcement learning. In: The 2010 International Joint Conference on Neural
  Networks (IJCNN). pp. 1--8.

\bibitem[{Meeussen et~al.(2010)Meeussen, Wise, Glaser, Chitta, McGann,
  Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski,
  Konolige, Gerkey, and Berger}]{peg_personal_icra_2010}
Meeussen, W., Wise, M., Glaser, S., Chitta, S., McGann, C., Mihelich, P.,
  Marder-Eppstein, E., Muja, M., Eruhimov, V., Foote, T., Hsu, J., Rusu, R.~B.,
  Marthi, B., Bradski, G., Konolige, K., Gerkey, B., Berger, E., May 2010.
  Autonomous door opening and plugging in with a personal robot. In: Robotics
  and Automation (ICRA), 2010 IEEE International Conference on. pp. 729--736.

\bibitem[{Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis}]{mnih-dqn-2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., Hassabis, D., 02 2015. Human-level control through deep
  reinforcement learning. Nature 518~(7540), 529--533.
\newline\urlprefix\url{http://dx.doi.org/10.1038/nature14236}

\bibitem[{Nemec et~al.(2013)Nemec, Abu-Dakka, Ridge, Ude, Jorgensen,
  Savarimuthu, Jouffroy, Petersen, and Kr\"uger}]{trans_workpiece_icra_2013}
Nemec, B., Abu-Dakka, F.~J., Ridge, B., Ude, A., Jorgensen, J.~A., Savarimuthu,
  T.~R., Jouffroy, J., Petersen, H.~G., Kr\"uger, N., Nov 2013. Transfer of
  assembly operations to new workpiece poses by adaptation to the desired force
  profile. In: Advanced Robotics (ICAR), 2013 16th International Conference on.
  pp. 1--7.

\bibitem[{Neumann and Peters(2009{\natexlab{a}})}]{fqi_nips_peter_2009}
Neumann, G., Peters, J., Jun. 2009{\natexlab{a}}. Fitted q-iteration by
  advantage weighted regression. In: Advances in neural information processing
  systems 21. pp. 1177--1184.

\bibitem[{Neumann and Peters(2009{\natexlab{b}})}]{NIPS2008_3501}
Neumann, G., Peters, J.~R., 2009{\natexlab{b}}. Fitted q-iteration by advantage
  weighted regression. In: Koller, D., Schuurmans, D., Bengio, Y., Bottou, L.
  (Eds.), Advances in Neural Information Processing Systems 21. Curran
  Associates, Inc., pp. 1177--1184.

\bibitem[{Ormoneit and Glynn(2002)}]{kernel_rl_ormoneit_2002}
Ormoneit, D., Glynn, P., Oct 2002. Kernel-based reinforcement learning in
  average-cost problems. IEEE Transactions on Automatic Control 47~(10),
  1624--1636.

\bibitem[{Peters and Schaal(2008{\natexlab{a}})}]{NAC_2008}
Peters, J., Schaal, S., 2008{\natexlab{a}}. Natural actor-critic. 15th European
  Symposium on Artificial Neural Networks 71~(7-9), 1180--1190.
\newline\urlprefix\url{http://www.sciencedirect.com/science/article/pii/S0925231208000532}

\bibitem[{Peters and Schaal(2008{\natexlab{b}})}]{peter_nac_2008}
Peters, J., Schaal, S., 2008{\natexlab{b}}. Natural actor-critic.
  Neurocomputing 71~(7-9), 1180--1190.

\bibitem[{Riedmiller(2005)}]{Riedmiller2005}
Riedmiller, M., 2005. Neural Fitted Q Iteration - First Experiences with a Data
  Efficient Neural Reinforcement Learning Method. pp. 317--328.
\newline\urlprefix\url{http://dx.doi.org/10.1007/11564096_32}

\bibitem[{Schaal et~al.(2004)Schaal, Peters, Nakanishi, and
  Ijspeert}]{Schaal04learningmovement}
Schaal, S., Peters, J., Nakanishi, J., Ijspeert, A., 2004. Learning movement
  primitives. In: International Symposium on Robotics Research (ISRR2003.
  Springer.

\bibitem[{Sung(2004)}]{gmr_2004}
Sung, H., 2004. Gaussian mixture regression and classification. Ph.D. thesis,
  Rice University.

\bibitem[{Sutton and Barto(1998)}]{sutton98a}
Sutton, R.~S., Barto, A.~G., 1998. Reinforcement Learning: An Introduction.
  {MIT} Press.
\newline\urlprefix\url{http://www.cs.ualberta.ca/\%7Esutton/book/ebook/the-book.html}

\bibitem[{Wiering and van Otterio(2012)}]{RL_state_art_2012}
Wiering, M., van Otterio, M., 2012. Reinforcement Learning State-of-the-Art.
  Springer-Verlag Berlin Heidelberg.

\bibitem[{Yang et~al.(2014)Yang, Lin, Song, Nemec, Ude, Buch, Krüger, and
  Savarimuthu}]{fast_peg_pbd_icmc_2014}
Yang, Y., Lin, L., Song, Y., Nemec, B., Ude, A., Buch, A., Krüger, N.,
  Savarimuthu, T., 2014. Fast programming of peg-in-hole actions by human
  demonstration. IEEE, pp. 990--995.

\end{thebibliography}
